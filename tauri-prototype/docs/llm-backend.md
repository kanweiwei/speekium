# LLM Backend Module Documentation

## Overview

The LLM Backend Module provides a unified interface for integrating various LLM backends into the Speekium Tauri application. It supports both Claude Code CLI and Ollama backends with streaming capabilities.

## Architecture

### Core Components

1. **LLMBackend (Abstract Base Class)**
   - Base interface for all LLM backends
   - Manages conversation history
   - Provides async streaming methods

2. **ClaudeBackend**
   - Integrates with Claude Code CLI
   - Uses subprocess for communication
   - Supports streaming JSON responses

3. **OllamaBackend**
   - Integrates with local Ollama server
   - Uses HTTP API (httpx)
   - Supports streaming responses

4. **LLMBackendManager**
   - Manages backend lifecycle
   - Integrates with configuration system
   - Handles backend switching

## File Structure

```
tauri-prototype/
├── src-python/
│   └── llm_backend.py        # Main LLM backend module
├── backend.py                # PyTauri command integration
└── test_llm_backend.py       # Test suite
```

## Usage

### 1. Basic Backend Creation

```python
from llm_backend import ClaudeBackend, OllamaBackend

# Create Claude backend
claude = ClaudeBackend(
    system_prompt="You are a helpful assistant",
    max_history=10
)

# Create Ollama backend
ollama = OllamaBackend(
    system_prompt="You are a helpful assistant",
    model="qwen2.5:1.5b",
    base_url="http://localhost:11434",
    max_history=10
)
```

### 2. Using Factory Function

```python
from llm_backend import create_backend

# Create backend by type
backend = create_backend(
    "ollama",
    system_prompt="You are a helpful assistant",
    model="qwen2.5:1.5b",
    base_url="http://localhost:11434",
    max_history=10
)
```

### 3. Async Streaming Chat

```python
async def chat_example():
    backend = create_backend("ollama", system_prompt="...")

    async for text, msg_type in backend.chat_stream_async("Hello!"):
        print(f"{msg_type}: {text}")

    # Cleanup
    await backend.shutdown()
```

### 4. Using LLMBackendManager

```python
from llm_backend import LLMBackendManager

# Create manager with config
config = {
    "llm_backend": "ollama",
    "ollama_model": "qwen2.5:1.5b",
    "ollama_base_url": "http://localhost:11434",
    "max_history": 10
}

manager = LLMBackendManager(config)
backend = manager.load_backend()

# Switch backends
manager.load_backend("claude")

# Clear history
await manager.clear_history()

# Shutdown
await manager.shutdown()
```

## PyTauri Integration

### Available Commands

#### `chat_generator`

Generate chat response with streaming:

```python
from pydantic import BaseModel

class ChatRequest(BaseModel):
    text: str
    language: str = "auto"
    history: Optional[List[Dict[str, Any]]] = None

# Command signature
async def chat_generator(body: ChatRequest, app_handle: AppHandle) -> List[ChatChunk]:
    # Returns list of ChatChunk objects
    pass
```

**ChatChunk Structure:**
```python
class ChatChunk(BaseModel):
    type: str  # "partial", "complete", "error"
    content: Optional[str] = None
    audio: Optional[str] = None  # Base64 audio (future)
    error: Optional[str] = None
```

#### `load_llm`

Load or reload LLM backend:

```python
class LoadLLMRequest(BaseModel):
    backend_type: Optional[str] = None

async def load_llm(body: LoadLLMRequest, app_handle: AppHandle) -> Dict[str, Any]:
    # Returns {"success": bool, "backend_type": str, "model": str}
    pass
```

#### `clear_history`

Clear conversation history:

```python
async def clear_history(app_handle: AppHandle) -> Dict[str, Any]:
    # Returns {"success": bool, "error": Optional[str]}
    pass
```

## Configuration

### Backend-Specific Config

**Claude Backend:**
```python
{
    "llm_backend": "claude",
    "max_history": 10
}
```

**Ollama Backend:**
```python
{
    "llm_backend": "ollama",
    "ollama_model": "qwen2.5:1.5b",
    "ollama_base_url": "http://localhost:11434",
    "max_history": 10
}
```

### System Prompt

The system prompt is automatically generated by LLMBackendManager:

```
You are Speekium, an intelligent voice assistant. Follow these rules:
1. Detect the user's language and respond in the same language
2. ONLY answer the current question - do not repeat or re-answer previous topics
3. Keep responses concise - 1-2 sentences unless more detail is requested
4. Use natural conversational style suitable for speech output
5. Never use markdown formatting, code blocks, or list symbols
6. Avoid special symbols like *, #, `, - etc.
7. Express numbers naturally (e.g., "three point five" instead of "3.5")
8. Be friendly, like chatting with a friend
```

## Streaming Behavior

### Claude Backend

1. Starts `claude` CLI with `--output-format stream-json`
2. Parses JSON events from stdout
3. Extracts text deltas from content blocks
4. Splits at sentence boundaries (。！？\n)
5. Yields sentences as `ChatMessageType.PARTIAL`
6. Sends full response as `ChatMessageType.COMPLETE`

### Ollama Backend

1. Opens HTTP POST stream to `/api/chat`
2. Parses JSON lines from response
3. Extracts content from message deltas
4. Splits at sentence boundaries (。！？\n)
5. Yields sentences as `ChatMessageType.PARTIAL`
6. Sends full response as `ChatMessageType.COMPLETE`

## Error Handling

All backends gracefully handle:

- **Timeouts**: Return error message after 120s timeout
- **Network Errors**: Catch and return error description
- **JSON Parse Errors**: Skip malformed JSON, continue processing
- **Process Errors**: Kill subprocess on error

## Testing

Run the test suite:

```bash
cd tauri-prototype
python test_llm_backend.py
```

Tests cover:
- ✅ Backend factory function
- ✅ Ollama streaming backend
- ✅ LLMBackendManager
- ✅ Conversation history management

## Performance Considerations

1. **ThreadPoolExecutor**: Blocking operations run in separate threads
2. **Async I/O**: All I/O operations use async/await
3. **Sentence Buffering**: Accumulates text until sentence boundaries
4. **Resource Cleanup**: Proper shutdown methods for all backends

## Future Enhancements

- [ ] Add OpenAI API backend
- [ ] Support for custom HTTP-based backends
- [ ] Add audio generation (TTS) integration
- [ ] Implement retry logic for transient errors
- [ ] Add request rate limiting
- [ ] Support for streaming audio responses

## Dependencies

```python
# Required
import asyncio
import json
import logging
import re
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from typing import AsyncIterator, List, Dict, Optional, Tuple
from pathlib import Path
from enum import Enum

# Optional (backend-specific)
import subprocess  # Claude
import httpx       # Ollama
```

## Troubleshooting

### Claude CLI not found

```bash
npm install -g @anthropic-ai/claude-code
```

### Ollama server not running

```bash
# Start Ollama
ollama serve

# Or pull model
ollama pull qwen2.5:1.5b
```

### Import errors

Ensure `src-python` is in path:

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "src-python"))
```

## Migration from backends.py

The new module maintains compatibility with the original `backends.py`:

| Old (backends.py) | New (llm_backend.py) |
|-------------------|----------------------|
| `chat(message: str) -> str` | `chat_async(message: str) -> Awaitable[str]` |
| `chat_stream(message: str)` | `chat_stream_async(message: str) -> AsyncIterator[Tuple[str, ChatMessageType]]` |
| Sync subprocess | Async subprocess with ThreadPoolExecutor |
| Direct stderr/stdout | Error handling via ChatMessageType.ERROR |

Key improvements:
- ✅ Fully async/await compatible
- ✅ Better error handling
- ✅ Structured streaming with message types
- ✅ Integrated with LLMBackendManager
- ✅ Compatible with PyTauri commands
