#!/usr/bin/env python3
"""
Speekium - æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹
é€šè¿‡è‡ªç„¶è¯­éŸ³ä¸å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯äº¤äº’
æµç¨‹: [VADæ£€æµ‹äººå£°] â†’ å½•éŸ³ â†’ SenseVoiceè¯†åˆ« â†’ LLMæµå¼å›å¤ â†’ è¾¹ç”Ÿæˆè¾¹æœ—è¯»

æ”¯æŒåç«¯: Claude Code CLI, Ollama
"""

import tempfile
import asyncio
import os
import re
import platform
from collections import deque
import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write as write_wav
import edge_tts
import torch

from backends import create_backend

# ===== LLM åç«¯é…ç½® =====
LLM_BACKEND = "claude"  # å¯é€‰: "claude", "ollama"

# Ollama é…ç½® (ä»…å½“ LLM_BACKEND="ollama" æ—¶ç”Ÿæ•ˆ)
OLLAMA_MODEL = "qwen2.5:1.5b"  # Ollama æ¨¡å‹åç§° (å¯é€‰: qwen2.5:7b æ›´æ™ºèƒ½ä½†æ›´æ…¢)
OLLAMA_BASE_URL = "http://localhost:11434"  # Ollama æœåŠ¡åœ°å€

# ===== åŸºç¡€é…ç½® =====
SAMPLE_RATE = 16000
ASR_MODEL = "iic/SenseVoiceSmall"  # SenseVoice æ¨¡å‹
TTS_VOICE = "zh-CN-XiaoyiNeural"  # å°è‰º
TTS_RATE = "+0%"  # è¯­é€Ÿ (è´Ÿå€¼å‡æ…¢ï¼Œæ­£å€¼åŠ å¿«ï¼Œ0%ä¸ºæ­£å¸¸)
USE_STREAMING = True  # æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆè¾¹ç”Ÿæˆè¾¹æœ—è¯»ï¼‰

# ===== VAD é…ç½® =====
VAD_THRESHOLD = 0.5  # è¯­éŸ³æ£€æµ‹é˜ˆå€¼
VAD_CONSECUTIVE_THRESHOLD = 3  # è¿ç»­æ£€æµ‹åˆ°è¯­éŸ³çš„æ¬¡æ•°æ‰ç¡®è®¤å¼€å§‹è¯´è¯
VAD_PRE_BUFFER = 0.3  # é¢„ç¼“å†²æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œä¿ç•™è¯­éŸ³å¼€å§‹å‰çš„éŸ³é¢‘
MIN_SPEECH_DURATION = 0.5  # æœ€çŸ­è¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
SILENCE_AFTER_SPEECH = 1.5  # è¯´å®Œåé™éŸ³å¤šä¹…åœæ­¢å½•éŸ³ï¼ˆç§’ï¼‰
MAX_RECORDING_DURATION = 30  # æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰

# ===== ç³»ç»Ÿæç¤ºè¯ï¼ˆä¼˜åŒ–è¯­éŸ³è¾“å‡ºï¼‰=====
SYSTEM_PROMPT = """ä½ æ˜¯ Speekium æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼Œè¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ç”¨å£è¯­åŒ–çš„ä¸­æ–‡å›ç­”ï¼Œé€‚åˆæœ—è¯»
2. ä¸è¦ä½¿ç”¨ markdown æ ¼å¼ã€ä»£ç å—ã€åˆ—è¡¨ç¬¦å·
3. ä¸è¦ä½¿ç”¨ç‰¹æ®Šç¬¦å·å¦‚ *ã€#ã€`ã€- ç­‰
4. æ•°å­—ç”¨ä¸­æ–‡è¡¨è¾¾ï¼Œå¦‚"ä¸‰ç‚¹äº”"è€Œä¸æ˜¯"3.5"
5. è¯­æ°”è‡ªç„¶å‹å¥½ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·"""


class VoiceAssistant:
    def __init__(self):
        self.asr_model = None
        self.vad_model = None
        self.llm_backend = None

    def load_asr(self):
        if self.asr_model is None:
            print("ğŸ”„ åŠ è½½ SenseVoice æ¨¡å‹...", flush=True)
            from funasr import AutoModel
            self.asr_model = AutoModel(model=ASR_MODEL, device="cpu")
            print("âœ… SenseVoice æ¨¡å‹åŠ è½½å®Œæˆ", flush=True)
        return self.asr_model

    def load_vad(self):
        if self.vad_model is None:
            print("ğŸ”„ åŠ è½½ VAD æ¨¡å‹...", flush=True)
            self.vad_model, _ = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                trust_repo=True
            )
            print("âœ… VAD æ¨¡å‹åŠ è½½å®Œæˆ", flush=True)
        return self.vad_model

    def load_llm(self):
        if self.llm_backend is None:
            print(f"ğŸ”„ åˆå§‹åŒ– LLM åç«¯ ({LLM_BACKEND})...", flush=True)
            if LLM_BACKEND == "ollama":
                self.llm_backend = create_backend(
                    LLM_BACKEND,
                    SYSTEM_PROMPT,
                    model=OLLAMA_MODEL,
                    base_url=OLLAMA_BASE_URL
                )
            else:
                self.llm_backend = create_backend(LLM_BACKEND, SYSTEM_PROMPT)
            print(f"âœ… LLM åç«¯åˆå§‹åŒ–å®Œæˆ", flush=True)
        return self.llm_backend

    def record_with_vad(self):
        """ä½¿ç”¨ VAD æ£€æµ‹è¯­éŸ³ï¼Œè‡ªåŠ¨å¼€å§‹å’Œåœæ­¢å½•éŸ³"""
        model = self.load_vad()
        model.reset_states()  # é‡ç½® VAD çŠ¶æ€

        print("\nğŸ‘‚ æ­£åœ¨è†å¬...", flush=True)

        chunk_size = 512  # Silero VAD éœ€è¦ 512 samples @ 16kHz
        frames = []
        is_speaking = False
        silence_chunks = 0
        speech_chunks = 0
        consecutive_speech = 0  # è¿ç»­æ£€æµ‹åˆ°è¯­éŸ³çš„æ¬¡æ•°
        max_silence_chunks = int(SILENCE_AFTER_SPEECH * SAMPLE_RATE / chunk_size)
        min_speech_chunks = int(MIN_SPEECH_DURATION * SAMPLE_RATE / chunk_size)
        max_chunks = int(MAX_RECORDING_DURATION * SAMPLE_RATE / chunk_size)

        # é¢„ç¼“å†²ï¼šä¿ç•™è¯­éŸ³å¼€å§‹å‰çš„éŸ³é¢‘ï¼Œé¿å…ä¸¢å¤±å¼€å¤´
        pre_buffer_size = int(VAD_PRE_BUFFER * SAMPLE_RATE / chunk_size)
        pre_buffer = deque(maxlen=pre_buffer_size)

        recording_done = False

        def callback(indata, frame_count, time_info, status):
            nonlocal is_speaking, silence_chunks, speech_chunks, consecutive_speech, recording_done

            if recording_done:
                return

            try:
                audio_chunk = indata[:, 0].copy()

                # VAD æ£€æµ‹
                audio_tensor = torch.from_numpy(audio_chunk).float()
                speech_prob = model(audio_tensor, SAMPLE_RATE).item()

                if speech_prob > VAD_THRESHOLD:
                    # æ£€æµ‹åˆ°è¯­éŸ³
                    consecutive_speech += 1

                    if not is_speaking and consecutive_speech >= VAD_CONSECUTIVE_THRESHOLD:
                        is_speaking = True
                        # å°†é¢„ç¼“å†²çš„éŸ³é¢‘æ·»åŠ åˆ° framesï¼Œé¿å…ä¸¢å¤±è¯­éŸ³å¼€å¤´
                        frames.extend(pre_buffer)
                        pre_buffer.clear()
                        print(f"ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...", flush=True)

                    if is_speaking:
                        # åªæœ‰è¿ç»­æ£€æµ‹åˆ°è¯­éŸ³æ‰é‡ç½®é™éŸ³è®¡æ•°
                        if consecutive_speech >= VAD_CONSECUTIVE_THRESHOLD:
                            silence_chunks = 0
                        speech_chunks += 1
                        frames.append(audio_chunk)
                    else:
                        # è¿˜æœªç¡®è®¤å¼€å§‹è¯´è¯ï¼Œç»§ç»­å¡«å……é¢„ç¼“å†²
                        pre_buffer.append(audio_chunk)
                else:
                    # é™éŸ³
                    consecutive_speech = 0  # é‡ç½®è¿ç»­è¯­éŸ³è®¡æ•°

                    if is_speaking:
                        frames.append(audio_chunk)
                        silence_chunks += 1

                        # è¯´å®Œåé™éŸ³è¶³å¤Ÿé•¿ï¼Œåœæ­¢å½•éŸ³
                        if silence_chunks >= max_silence_chunks and speech_chunks >= min_speech_chunks:
                            recording_done = True
                            print("ğŸ”‡ è¯­éŸ³ç»“æŸ", flush=True)
                    else:
                        # è¿˜æœªå¼€å§‹è¯´è¯ï¼Œç»§ç»­å¡«å……é¢„ç¼“å†²
                        pre_buffer.append(audio_chunk)

                # è¶…è¿‡æœ€å¤§æ—¶é•¿
                if len(frames) >= max_chunks:
                    recording_done = True
                    print("â±ï¸ è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿", flush=True)

            except Exception as e:
                print(f"âš ï¸ VAD å¤„ç†é”™è¯¯: {e}", flush=True)
                recording_done = True

        with sd.InputStream(
            samplerate=SAMPLE_RATE, channels=1, dtype=np.float32,
            blocksize=chunk_size, callback=callback
        ):
            while not recording_done:
                sd.sleep(50)

        if not frames or speech_chunks < min_speech_chunks:
            return None

        audio = np.concatenate(frames)
        print(f"âœ… å½•éŸ³å®Œæˆ ({len(audio)/SAMPLE_RATE:.1f}ç§’)", flush=True)
        return audio

    def transcribe(self, audio):
        print("ğŸ”„ è¯†åˆ«ä¸­...", flush=True)
        model = self.load_asr()
        tmp_file = None

        try:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                tmp_file = f.name
                audio_int16 = (audio * 32767).astype(np.int16)
                write_wav(tmp_file, SAMPLE_RATE, audio_int16)
                result = model.generate(input=tmp_file)
                text = result[0]["text"] if result else ""
        finally:
            if tmp_file and os.path.exists(tmp_file):
                os.remove(tmp_file)

        # æ¸…ç† SenseVoice è¾“å‡ºçš„æ ‡ç­¾ï¼Œå¦‚ <|yue|><|EMO_UNKNOWN|><|Speech|>
        text = re.sub(r'<\|[^|]+\|>', '', text).strip()

        print(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}", flush=True)
        return text

    async def generate_audio(self, text):
        """ç”Ÿæˆ TTS éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„"""
        try:
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
                tmp_file = f.name
                communicate = edge_tts.Communicate(text, TTS_VOICE, rate=TTS_RATE)
                await communicate.save(tmp_file)
                return tmp_file
        except Exception as e:
            print(f"âš ï¸ TTS ç”Ÿæˆå¤±è´¥: {e}", flush=True)
            return None

    async def play_audio(self, tmp_file, delete=True):
        """æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼Œè·¨å¹³å°ï¼‰ï¼Œå¯é€‰æ˜¯å¦åˆ é™¤"""
        if tmp_file and os.path.exists(tmp_file):
            try:
                system = platform.system()
                if system == "Darwin":  # macOS
                    cmd = ["afplay", tmp_file]
                elif system == "Linux":
                    cmd = ["ffplay", "-nodisp", "-autoexit", "-loglevel", "quiet", tmp_file]
                elif system == "Windows":
                    cmd = ["powershell", "-c", f"(New-Object Media.SoundPlayer '{tmp_file}').PlaySync()"]
                else:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„å¹³å°: {system}", flush=True)
                    return

                process = await asyncio.create_subprocess_exec(*cmd)
                await process.wait()
            finally:
                if delete:
                    os.remove(tmp_file)

    async def speak(self, text):
        """TTS æœ—è¯»ï¼ˆå•å¥ï¼‰"""
        tmp_file = await self.generate_audio(text)
        await self.play_audio(tmp_file)

    async def chat_once(self):
        """å•æ¬¡å¯¹è¯"""
        audio = self.record_with_vad()

        if audio is None:
            return False  # æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³

        text = self.transcribe(audio)
        if not text:
            print("âš ï¸  æœªè¯†åˆ«åˆ°å†…å®¹", flush=True)
            return True

        backend = self.load_llm()

        if USE_STREAMING:
            # æµå¼è¾“å‡º
            print("ğŸ”Š æµå¼æœ—è¯»ä¸­...", flush=True)
            audio_queue = asyncio.Queue()

            async def generate_worker():
                async for sentence in backend.chat_stream(text):
                    if sentence:
                        audio_file = await self.generate_audio(sentence)
                        if audio_file:
                            await audio_queue.put(audio_file)
                await audio_queue.put(None)

            async def play_worker():
                while True:
                    audio_file = await audio_queue.get()
                    if audio_file is None:
                        break
                    await self.play_audio(audio_file)

            await asyncio.gather(generate_worker(), play_worker())
        else:
            # éæµå¼è¾“å‡º
            response = backend.chat(text)
            await self.speak(response)

        return True

    async def run(self):
        print("=" * 50, flush=True)
        print("ğŸ™ï¸  Speekium å·²å¯åŠ¨ (æŒç»­å¯¹è¯æ¨¡å¼)", flush=True)
        print("   ä½¿ç”¨ VAD è‡ªåŠ¨æ£€æµ‹è¯­éŸ³", flush=True)
        backend_info = LLM_BACKEND
        if LLM_BACKEND == "ollama":
            backend_info = f"ollama ({OLLAMA_MODEL})"
        print(f"   LLM åç«¯: {backend_info}", flush=True)
        if USE_STREAMING:
            print("   æ¨¡å¼: æµå¼è¾“å‡ºï¼ˆè¾¹ç”Ÿæˆè¾¹æœ—è¯»ï¼‰", flush=True)
        print("   Ctrl+C é€€å‡º", flush=True)
        print("=" * 50, flush=True)

        # é¢„åŠ è½½æ¨¡å‹
        self.load_vad()
        self.load_asr()
        self.load_llm()

        print("\nğŸ§ å‡†å¤‡å°±ç»ªï¼Œè¯·å¼€å§‹è¯´è¯...\n", flush=True)

        try:
            while True:
                await self.chat_once()
                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…ç«‹å³å¼€å§‹ä¸‹ä¸€è½®
                await asyncio.sleep(0.5)

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!", flush=True)


async def main():
    assistant = VoiceAssistant()
    await assistant.run()


if __name__ == "__main__":
    asyncio.run(main())
