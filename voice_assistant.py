#!/usr/bin/env python3
"""
è¯­éŸ³åŠ©æ‰‹ - å¯¹æ¥ Claude Code CLI
æµç¨‹: [å”¤é†’è¯/æŒ‰é”®] â†’ å½•éŸ³ â†’ Whisperè¯†åˆ« â†’ Claudeå›å¤ â†’ Edge TTSæœ—è¯»
"""

import subprocess
import tempfile
import asyncio
import sys
import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write as write_wav

# é…ç½®
SAMPLE_RATE = 16000
WHISPER_MODEL = "base"  # tiny/base/small/medium/large
TTS_VOICE = "zh-CN-XiaoyiNeural"  # å°è‰º
TTS_RATE = "-15%"  # è¯­é€Ÿ
USE_WAKE_WORD = False  # æ˜¯å¦ä½¿ç”¨å”¤é†’è¯ï¼ˆTrue=å”¤é†’è¯ï¼ŒFalse=æŒ‰å›è½¦ï¼‰
WAKE_WORD = "hey_jarvis"
WAKE_THRESHOLD = 0.5


class VoiceAssistant:
    def __init__(self):
        self.whisper_model = None
        self.wake_model = None

    def load_whisper(self):
        if self.whisper_model is None:
            print("ğŸ”„ åŠ è½½ Whisper æ¨¡å‹...", flush=True)
            from faster_whisper import WhisperModel
            self.whisper_model = WhisperModel(WHISPER_MODEL, compute_type="int8")
            print("âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆ", flush=True)
        return self.whisper_model

    def load_wake_model(self):
        if self.wake_model is None:
            print("ğŸ”„ åŠ è½½å”¤é†’è¯æ¨¡å‹...", flush=True)
            from openwakeword.model import Model
            self.wake_model = Model(
                wakeword_models=[WAKE_WORD],
                inference_framework='onnx'
            )
            print(f"âœ… å”¤é†’è¯æ¨¡å‹åŠ è½½å®Œæˆ", flush=True)
        return self.wake_model

    def wait_for_wake_word(self):
        """ç­‰å¾…å”¤é†’è¯"""
        model = self.load_wake_model()
        print(f"\nğŸ‘‚ ç­‰å¾…å”¤é†’è¯ 'Hey Jarvis'...", flush=True)

        chunk_size = 1280
        detected = False

        def callback(indata, frames, time_info, status):
            nonlocal detected
            if detected:
                return
            audio = indata[:, 0]
            prediction = model.predict(audio)
            score = prediction[WAKE_WORD]
            if score > WAKE_THRESHOLD:
                detected = True
                print(f"âœ¨ æ£€æµ‹åˆ°å”¤é†’è¯! (ç½®ä¿¡åº¦: {score:.2f})", flush=True)

        with sd.InputStream(
            samplerate=SAMPLE_RATE, channels=1, dtype=np.float32,
            blocksize=chunk_size, callback=callback
        ):
            while not detected:
                sd.sleep(100)

        model.reset()
        return True

    def wait_for_key(self):
        """ç­‰å¾…æŒ‰é”®"""
        input("\næŒ‰å›è½¦å¼€å§‹å½•éŸ³...")
        return True

    def record_audio_manual(self):
        """å½•éŸ³ - æŒ‰å›è½¦åœæ­¢"""
        print("ğŸ¤ å¼€å§‹å½•éŸ³... (æŒ‰å›è½¦åœæ­¢)", flush=True)

        frames = []
        recording = True

        def callback(indata, frame_count, time_info, status):
            if recording:
                frames.append(indata.copy())

        stream = sd.InputStream(
            samplerate=SAMPLE_RATE, channels=1, dtype=np.float32, callback=callback
        )

        with stream:
            input()
            recording = False

        if not frames:
            return None

        audio = np.concatenate(frames, axis=0)
        print(f"âœ… å½•éŸ³å®Œæˆ ({len(audio)/SAMPLE_RATE:.1f}ç§’)", flush=True)
        return audio

    def record_audio_auto(self, max_duration=10, silence_threshold=0.01, silence_duration=1.5):
        """å½•éŸ³ - é™éŸ³è‡ªåŠ¨åœæ­¢"""
        print("ğŸ¤ è¯·è¯´è¯... (é™éŸ³è‡ªåŠ¨åœæ­¢)", flush=True)

        frames = []
        silent_chunks = 0
        chunk_duration = 0.1
        max_silent_chunks = int(silence_duration / chunk_duration)

        def callback(indata, frame_count, time_info, status):
            nonlocal silent_chunks
            frames.append(indata.copy())
            volume = np.abs(indata).mean()
            if volume < silence_threshold:
                silent_chunks += 1
            else:
                silent_chunks = 0

        with sd.InputStream(
            samplerate=SAMPLE_RATE, channels=1, dtype=np.float32,
            callback=callback, blocksize=int(SAMPLE_RATE * chunk_duration)
        ):
            while True:
                sd.sleep(100)
                if silent_chunks >= max_silent_chunks and len(frames) > 10:
                    print("ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³ï¼Œåœæ­¢å½•éŸ³", flush=True)
                    break
                if len(frames) * chunk_duration >= max_duration:
                    print("â±ï¸ è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿", flush=True)
                    break

        if not frames:
            return None

        audio = np.concatenate(frames, axis=0)
        print(f"âœ… å½•éŸ³å®Œæˆ ({len(audio)/SAMPLE_RATE:.1f}ç§’)", flush=True)
        return audio

    def transcribe(self, audio):
        print("ğŸ”„ è¯†åˆ«ä¸­...", flush=True)
        model = self.load_whisper()

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            audio_int16 = (audio * 32767).astype(np.int16)
            write_wav(f.name, SAMPLE_RATE, audio_int16)
            segments, info = model.transcribe(f.name, language="zh")
            text = "".join([seg.text for seg in segments])

        print(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}", flush=True)
        return text.strip()

    def ask_claude(self, question):
        print("ğŸ¤– Claude æ€è€ƒä¸­...", flush=True)
        try:
            result = subprocess.run(
                ["claude", "-p", question],
                capture_output=True, text=True, timeout=60
            )
            response = result.stdout.strip()
            print(f"ğŸ’¬ Claude: {response[:200]}{'...' if len(response) > 200 else ''}", flush=True)
            return response
        except subprocess.TimeoutExpired:
            return "æŠ±æ­‰ï¼Œå›å¤è¶…æ—¶äº†"
        except Exception as e:
            return f"å‡ºé”™äº†: {e}"

    async def speak(self, text):
        import edge_tts
        print("ğŸ”Š æœ—è¯»ä¸­...", flush=True)
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            communicate = edge_tts.Communicate(text, TTS_VOICE, rate=TTS_RATE)
            await communicate.save(f.name)
            subprocess.run(["afplay", f.name])

    async def chat_once(self):
        # å½•éŸ³
        if USE_WAKE_WORD:
            audio = self.record_audio_auto()
        else:
            audio = self.record_audio_manual()

        if audio is None or len(audio) < SAMPLE_RATE * 0.5:
            print("âš ï¸  å½•éŸ³å¤ªçŸ­ï¼Œè·³è¿‡", flush=True)
            return

        text = self.transcribe(audio)
        if not text:
            print("âš ï¸  æœªè¯†åˆ«åˆ°å†…å®¹", flush=True)
            return

        response = self.ask_claude(text)
        await self.speak(response)

    async def run(self):
        print("=" * 50, flush=True)
        print("ğŸ™ï¸  è¯­éŸ³åŠ©æ‰‹å·²å¯åŠ¨", flush=True)
        if USE_WAKE_WORD:
            print(f"   å”¤é†’è¯: 'Hey Jarvis'", flush=True)
        else:
            print("   æŒ‰å›è½¦å¼€å§‹å½•éŸ³ï¼Œå†æŒ‰å›è½¦åœæ­¢", flush=True)
        print("   Ctrl+C é€€å‡º", flush=True)
        print("=" * 50, flush=True)

        # é¢„åŠ è½½æ¨¡å‹
        if USE_WAKE_WORD:
            self.load_wake_model()
        self.load_whisper()

        try:
            while True:
                if USE_WAKE_WORD:
                    self.wait_for_wake_word()
                    await self.speak("æˆ‘åœ¨")
                else:
                    self.wait_for_key()

                await self.chat_once()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!", flush=True)


async def main():
    assistant = VoiceAssistant()
    await assistant.run()


if __name__ == "__main__":
    asyncio.run(main())
